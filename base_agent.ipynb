{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c48c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -qU langchain-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa282eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (1.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (0.4.37)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (2.1.2)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain) (1.11.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\morri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install -U langchain langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers langchain_ollama owlready2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec0c2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_splitter_llm(story, llm):\n",
    "    story_to_scenarios_prompt = f\"\"\"\n",
    "    You are an assistant that extracts logical or factual scenarios from a given story.\n",
    "\n",
    "    A scenario is a small, self-contained statement or short passage that expresses one or more closely related facts, events, or claims that can later be checked for correctness or consistency using an ontology.\n",
    "    ---\n",
    "\n",
    "    ### Guidelines:\n",
    "    - Each scenario should capture a complete idea, including all details that are logically connected (for example, cause–effect, contrast, or relationship).\n",
    "    - If two or more statements are relevant to each other (e.g., one qualifies, contradicts, or explains the other), combine them into one scenario.\n",
    "    - Preserve contextual and relational details - who, what, where, when, and why.\n",
    "    - Avoid redundancy - do not restate the same information.\n",
    "    - Keep each scenario brief but complete (usually one to three sentences).\n",
    "    - Include implicit facts when they are important (e.g., “John’s wife Amira” implies John is married to Amira).\n",
    "    - Ensure that every piece of relevant information from the story appears in at least one scenario.\n",
    "\n",
    "    ---\n",
    "\n",
    "    Example\n",
    "\n",
    "    Story:\n",
    "    “John is 15 years old and is on vacation with his wife Amira in Italy. Their daughter Anna can’t wait to visit the Eiffel Tower.”\n",
    "\n",
    "    Extracted Scenarios:\n",
    "    1. John is 15 years old and is married to Amira.\n",
    "    2. John and Amira are on vacation in Italy.\n",
    "    3. John and Amira have a daughter named Anna.\n",
    "    4. Anna wants to visit the Eiffel Tower.\n",
    "\n",
    "    ---\n",
    "\n",
    "    Now extract the scenarios from the following story and number them clearly:\n",
    "\n",
    "    {story}\n",
    "    \"\"\"\n",
    "\n",
    "    # Query the LLM\n",
    "    response = llm.invoke(story_to_scenarios_prompt)\n",
    "    if hasattr(response, \"content\"):  # LangChain AIMessage\n",
    "        output_text = response.content\n",
    "    elif isinstance(response, dict) and \"content\" in response:\n",
    "        output_text = response[\"content\"]\n",
    "    elif isinstance(response, str):\n",
    "        output_text = response\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected LLM response type: {type(response)}\")\n",
    "\n",
    "    # Extract list items using regex\n",
    "    import re\n",
    "    scenarios = re.findall(r'(?:\\d+\\.\\s*)(.+)', output_text)\n",
    "    scenarios = [s.strip() for s in scenarios if s.strip()]\n",
    "\n",
    "    # If the LLM doesnt number them, fallback to line splitting\n",
    "    if not scenarios:\n",
    "        scenarios = [line.strip(\"-• \\t\") for line in output_text.splitlines() if line.strip()]\n",
    "\n",
    "    return scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65c7cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def incremental_reasoning_no_rag(scenarios, custom_prompt, llm):\n",
    "    \"\"\"\n",
    "    Performs step-by-step reasoning on a sequence of scenarios, using only the\n",
    "    accumulated story so far (no ontology/vector store retrieval).\n",
    "    \"\"\"\n",
    "    updated_scenarios = []\n",
    "    results = []\n",
    "    accumulated_story = \"\"  # stores all accepted or corrected story so far\n",
    "\n",
    "    for i, scenario in enumerate(scenarios, start=1):\n",
    "        # 1. Construct reasoning context (story so far only)\n",
    "        full_context = f\"Story so far:\\n{accumulated_story.strip() or '(none so far)'}\\n\"\n",
    "\n",
    "        # 2. Build the complete prompt using the template\n",
    "        rendered_prompt = custom_prompt.invoke({\n",
    "            \"question\": scenario.strip(),\n",
    "            \"context\": full_context.strip(),\n",
    "        })\n",
    "\n",
    "        # 3. Invoke the LLM\n",
    "        llm_response = llm.invoke(rendered_prompt)\n",
    "        response_text = llm_response.content.strip()\n",
    "\n",
    "        # 4. Parse the model output (Consistency + Updated scenario)\n",
    "        updated_scenario = scenario.strip()\n",
    "        consistency = \"Unknown\"\n",
    "\n",
    "        for line in response_text.splitlines():\n",
    "            line_lower = line.strip().lower()\n",
    "            if line_lower.startswith(\"updated scenario:\"):\n",
    "                updated_scenario = line.split(\":\", 1)[1].strip()\n",
    "            elif line_lower.startswith(\"consistency:\"):\n",
    "                consistency = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "        # 6. Check consistency, update if not consistent\n",
    "        #If consistent, use unchanged scenario\n",
    "        if(consistency.lower().strip() == \"consistent\"):\n",
    "            #Update accumulated story so future reasoning uses this to fix scenarios\n",
    "            accumulated_story = (accumulated_story + \"\\n\" + scenario).strip()\n",
    "            #Update scenario\n",
    "            updated_scenarios.append(scenario)\n",
    "\n",
    "        #If not consistent, use llm updated scenario\n",
    "        else:\n",
    "            #Update accumulated story so future reasoning uses this to fix scenarios\n",
    "            accumulated_story = (accumulated_story + \"\\n\" + updated_scenario).strip()\n",
    "            #Update scenario\n",
    "            updated_scenarios.append(updated_scenario)\n",
    "\n",
    "        # 7. Store reasoning results\n",
    "        results.append({\n",
    "            \"step\": i,\n",
    "            \"original_scenario\": scenario.strip(),\n",
    "            \"updated_scenario\": updated_scenario,\n",
    "            \"consistency\": consistency,\n",
    "            \"llm_raw_output\": response_text.strip(),\n",
    "            \"story_so_far\": accumulated_story.strip(),\n",
    "        })\n",
    "\n",
    "    return updated_scenarios, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bf447fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_fixing_prompt_no_rag = PromptTemplate.from_template(\"\"\"\n",
    "    You are a careful reasoning assistant that ensures each scenario in a story\n",
    "    is logically consistent with the previously verified parts of the same story.\n",
    "\n",
    "    You will be given:\n",
    "    1) The story so far - previously verified or corrected facts.\n",
    "    2) A new scenario - the next statement to check.\n",
    "\n",
    "    Your task:\n",
    "    - Determine whether the new scenario is **consistent** with the story so far.\n",
    "    - If inconsistent, rewrite the scenario minimally to make it consistent.\n",
    "    - Prefer small, natural edits that preserve meaning and story flow.\n",
    "\n",
    "    **STRICT OUTPUT FORMAT (must follow exactly)**\n",
    "\n",
    "    Consistency: [Consistent / Inconsistent]\n",
    "\n",
    "    Updated scenario: [Corrected or unchanged scenario; if unchanged, copy the original. DO NOT OUTPUT \"No change\" or \"unchanged\"]\n",
    "\n",
    "    Story so far:\n",
    "    John is 15 years old.\n",
    "\n",
    "    Next scenario:\n",
    "    John is married to Amira.\n",
    "\n",
    "    Expected output:                                                   \n",
    "    Let's think step-by-step.\n",
    "    Explanation:                                                   \n",
    "    Consistency: Inconsistent\n",
    "    Updated scenario: John is 23 years old and is married to Amira.\n",
    "\n",
    "    Now analyze using the provided context below.\n",
    "    {context}\n",
    "\n",
    "    Next scenario:\n",
    "    {question}\n",
    "                                                        \n",
    "    Let's think step-by-step\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99cfd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "fix_story_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a reasoning assistant tasked with fixing inconsistencies in a story.\n",
    "\n",
    "You are given:\n",
    "\n",
    "The original story:\n",
    "{original_story}\n",
    "                                           \n",
    "The original inconsistent scenarios/facts taken from the original story\n",
    "{scenarios}\n",
    "\n",
    "A list of updated, consistent scenarios/facts:\n",
    "{updated_scenarios}\n",
    "\n",
    "Your task is to rewrite the new updated scenarios into the original story. Make minimal changes to the original story; preserve the writing style, narrative flow, and character details as much as possible. \n",
    "\n",
    "Return only the fixed story.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc0e4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_story(llm, story, scenarios, updated_scenarios):\n",
    "    fixed_story = llm.invoke(fix_story_prompt.format(\n",
    "    original_story=story,\n",
    "    scenarios=scenarios,\n",
    "    updated_scenarios=\"\\n\".join(updated_scenarios)\n",
    "))\n",
    "    return fixed_story.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38737283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict, Optional, Dict, Any\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "class StoryStateNoRAG(TypedDict, total=False):\n",
    "    story: str\n",
    "    scenarios: List[str]\n",
    "    updated_scenarios: List[str]\n",
    "    results: Any\n",
    "    fixed_story: str\n",
    "    llm: Any\n",
    "    loop_count: int #how many times check consistency node has been done\n",
    "\n",
    "#Create scenarions based on story\n",
    "def split_scenarios_node_no_rag(state: StoryStateNoRAG) -> Dict[str, Any]:\n",
    "    story = state.get(\"story\", \"\")\n",
    "    llm = state.get(\"llm\")\n",
    "    scenarios = scenario_splitter_llm(story, llm)\n",
    "    return {\"scenarios\": scenarios}\n",
    "\n",
    "#Fixes the scenarios incrementally\n",
    "def check_consistency_node_no_rag(state: StoryStateNoRAG) -> Dict[str, Any]:\n",
    "    scenarios = state.get(\"updated_scenarios\") or state.get(\"scenarios\")\n",
    "    llm = state.get(\"llm\")\n",
    "    loop_count = state.get(\"loop_count\", 0)\n",
    "\n",
    "    updated_scenarios, results = incremental_reasoning_no_rag(scenarios, scenario_fixing_prompt_no_rag, llm)\n",
    "    return {\"updated_scenarios\" : updated_scenarios, \"results\": results, \"loop_count\": loop_count + 1}\n",
    "\n",
    "#Fixes the story\n",
    "def fix_story_node_no_rag(state: StoryStateNoRAG) -> Dict[str, Any]:\n",
    "    story = state.get(\"story\", \"\")\n",
    "    scenarios = state.get(\"scenarios\")\n",
    "    updated_scenarios = state.get(\"updated_scenarios\")\n",
    "    llm = state.get(\"llm\")\n",
    "    fixed_story = fix_story(llm, story, scenarios, updated_scenarios)\n",
    "\n",
    "    return { \"fixed_story\": fixed_story}\n",
    "\n",
    "#condition for looping consistency check\n",
    "def should_repeat_check_consistency(state: StoryStateNoRAG):\n",
    "    #Limit to N loops\n",
    "    if state[\"loop_count\"] < 2:#Do total of 2 times\n",
    "        return \"repeat\"\n",
    "    return \"done\"\n",
    "\n",
    "builder_no_rag = StateGraph(StoryStateNoRAG)\n",
    "builder_no_rag.add_node(\"split_scenarios_no_rag\", split_scenarios_node_no_rag)\n",
    "builder_no_rag.add_node(\"check_consistency_no_rag\", check_consistency_node_no_rag)\n",
    "builder_no_rag.add_node(\"fix_story_no_rag\", fix_story_node_no_rag)\n",
    "\n",
    "builder_no_rag.add_edge(START, \"split_scenarios_no_rag\")\n",
    "builder_no_rag.add_edge(\"split_scenarios_no_rag\", \"check_consistency_no_rag\")\n",
    "builder_no_rag.add_conditional_edges(\n",
    "    \"check_consistency_no_rag\",\n",
    "    should_repeat_check_consistency,\n",
    "    {\n",
    "        \"repeat\": \"check_consistency_no_rag\", #loop\n",
    "        \"done\": \"fix_story_no_rag\",           #continue\n",
    "    },\n",
    ")\n",
    "builder_no_rag.add_edge(\"fix_story_no_rag\", END)\n",
    "\n",
    "graph_no_rag = builder_no_rag.compile()\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Stories. \n",
    "#   Comment out all other stories you dont use.\n",
    "#-------------------------------------------------\n",
    "\n",
    "#Story 1\n",
    "#story = \"\"\"John is 15 years old and is on vacation with his wife Amira in Italy. Their daughter Anna can’t wait to visit the famous landmark of Florance, the Eiffel Tower. After that they will go out to eat. John suggests they eat pizzas since Italy is famous for them, they will eat at restaurant Riccolo located in Florance.\n",
    "#Since Anna has a vitamin C deficiency she will order a pizza that contains tomatoes for vitamin C, so she will get the pizza bianca. John will get the classic Margherita pizza and Amira orders a pepperoni pizza.\n",
    "#They sit by the window of the small restaurant the air filled with the smell of garlic and baking dough. Anna swings her legs impatiently under the table, still talking about the Eiffel Tower, while Amira flips through a guidebook about Florance.\n",
    "#15 minutes later the farmer named Leo from the restaurant brings their pizzas and they eat, and Anna says to John how cool it is that the waiter is from France.\"\"\"\n",
    "\n",
    "#Story 2\n",
    "story = \"Jack and his family are celebrating his birthday, Jack just turned 235 years old. Jack loves talking about himself and everything he has done in his life, thats why many people would consider Jack a reserved person. This year, Jack’s family decided to visit the beautiful city of Quito, famous for its landmark La Virgen del Panecillo, which overlooks the city from a tall hill. Since the city doesn’t have mountainous terrain and is easy to walk through, Jack enjoys strolling through the streets without any trouble. It’s quite convenient, because Jack can’t walk very far due to his asthma. Luckily, Jack brought his medication for his asthma, so whenever he’s out of breath, he can simply take his antihistamine pills.\"\n",
    "\n",
    "#Story 3\n",
    "#story = \"\"\"Emily and her son Michael decided to adopt a cat from the local shelter in Denver, a walkable city surrounded by tall mountains. The city’s most famous landmark, the Red Rocks Amphitheatre, could be seen from their apartment window. Michael was thrilled to finally have a pet to take on walks through the flat city streets. The shelter worker handed them a small brown dog named Whiskers, explaining that he meows loudly when he wants food.\n",
    "#Later that afternoon, Michael went to his scheduled health check-up for obesity, which the doctor said was likely caused by living in Denver. After the appointment, Michael decided to walk Whiskers around the flat, car-free neighborhood for some exercise. Denver has a large population, which makes it one of the largest cities in the USA. Michael finds Denver the quietest city he has ever been to.\n",
    "#\"\"\"\n",
    "#Story 4\n",
    "#story = \"Jason loves to work outside on the Farm, which is why he became a surgeon. He and his family live just outside of Barcelona in France where he grows apples. He is a father of his daughter Amelie. When they visit the city Amelie gets very excited for all the vegan food, such as Chicken Tikka Masala, since she is vegetarian by choice but also lactose intolerant. Whenever they visit, they take their pet alligator Sally since she loves to meet other pets and is very social.\"\n",
    "\n",
    "#Story 5\n",
    "#story = \"\"\"Lara traveled to Rome with her friend Amira to enjoy Italian cuisine, they were excited to try everything. However, it wasn’t easy since Lara was vegetarian, and her best friend was gluten-free. They finally both enjoyed the Pizza and Pasta. They also explored the beautiful city, walking everywhere they could. Lara had to use her EpiPen daily since she was highly allergic to animal hair and in many places they encountered street animals, such as cats and dogs.\n",
    "#Lara loved the smell of baked bread from every corner, even though her friend couldn’t try any. When Lara returned home, she wanted to make food that everyone could enjoy, so she opened a small bakery and decided it would be completely gluten-free. Her favourite creation was bread made from rye.\n",
    "#\"\"\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    #model=\"phi3:mini\",\n",
    "    model = \"llama3.1\"\n",
    ")\n",
    "\n",
    "initial_state: StoryStateNoRAG = {\n",
    "    \"story\": story,\n",
    "    \"llm\": llm,\n",
    "    \"loop_count\": 0\n",
    "}\n",
    "from IPython.display import Image, display\n",
    "final_state = graph_no_rag.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2d4ff99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack is 235 years old and has a family that celebrates his birthday.',\n",
       " 'Jack is considered a reserved person due to his love for talking about himself.',\n",
       " \"Jack's family decided to visit the city of Quito, Ecuador this year.\",\n",
       " 'La Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.',\n",
       " 'The city of Quito has flat terrain and is easy to walk through.',\n",
       " 'Jack has asthma, which makes it difficult for him to walk far.',\n",
       " 'Jack brought his medication (antihistamine pills) with him on the trip to manage his asthma symptoms.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print Created scenarios from Story\n",
    "final_state[\"scenarios\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fd66957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack is 115 years old and has a family that celebrates his birthday.',\n",
       " '',\n",
       " \"Jack's family decided to visit the city of Quito, Ecuador this year.\",\n",
       " 'La Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.',\n",
       " \"La Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill, which contrasts with Quito's generally flat terrain and easy walkability.\",\n",
       " 'Jack has asthma, which makes it difficult for him to walk far.',\n",
       " 'Jack brought inhalers with him on the trip to manage his asthma symptoms.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print Updated scenarios from Story\n",
    "final_state[\"updated_scenarios\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1689e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the rewritten story with the inconsistencies fixed:\n",
      "\n",
      "Jack and his family are celebrating his birthday, Jack just turned 115 years old. Jack loves talking about himself and everything he has done in his life, that's why many people would consider Jack a reserved person. This year, Jack's family decided to visit the beautiful city of Quito, famous for its landmark La Virgen del Panecillo, which overlooks the city from a tall hill. Although the city doesn't have mountainous terrain and is easy to walk through, Jack has asthma, making it difficult for him to walk far. Luckily, Jack brought his inhalers with him on the trip to manage his asthma symptoms.\n",
      "\n",
      "Despite this limitation, Jack enjoys strolling through the streets without any trouble, taking regular breaks whenever he's out of breath. It's quite convenient, thanks to Quito's generally flat terrain that allows for easy walking, and La Virgen del Panecillo can be seen from a distance on one of these walks.\n"
     ]
    }
   ],
   "source": [
    "#Print Final fixed Story\n",
    "print(final_state[\"fixed_story\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcae7497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'step': 1,\n",
       "  'original_scenario': 'Jack is 235 years old and has a family that celebrates his birthday.',\n",
       "  'updated_scenario': 'Jack is 115 years old and has a family that celebrates his birthday.',\n",
       "  'consistency': 'Inconsistent',\n",
       "  'llm_raw_output': \"Consistency: Inconsistent\\n\\nUpdated scenario: Jack is 115 years old and has a family that celebrates his birthday.\\n\\nExplanation: \\nA human cannot live up to 235 years, as the maximum recorded age in history is about 122 years (Jeanne Calment). Therefore, the original statement is inconsistent with known facts. To make it consistent, I have revised Jack's age to be within the biologically possible range.\",\n",
       "  'story_so_far': 'Jack is 115 years old and has a family that celebrates his birthday.'},\n",
       " {'step': 2,\n",
       "  'original_scenario': 'Jack is considered a reserved person due to his love for talking about himself.',\n",
       "  'updated_scenario': '',\n",
       "  'consistency': 'Inconsistent',\n",
       "  'llm_raw_output': 'To ensure consistency, let\\'s analyze the given context:\\n\\nStory so far: \\nJack is 115 years old and has a family that celebrates his birthday.\\nNext scenario:\\nJack is considered a reserved person due to his love for talking about himself.\\n\\nIt seems contradictory for someone to be \"reserved\" (which means shy or uncommunicative) yet have a \"love for talking about himself.\" The two descriptions don\\'t align. A person who loves to talk about themselves would likely be outgoing, not reserved.\\n\\nConsistency: Inconsistent\\n\\nTo resolve the inconsistency, we\\'ll need to make a minimal edit to maintain the story\\'s coherence and flow. Let\\'s change Jack\\'s age from 115 to a more suitable number for someone with an active personality.\\n\\nUpdated scenario:\\nJack is 60 years old and has a family that celebrates his birthday.\\n\\nThis revised scenario removes the contradiction while keeping the core elements of the original statement intact.',\n",
       "  'story_so_far': 'Jack is 115 years old and has a family that celebrates his birthday.'},\n",
       " {'step': 3,\n",
       "  'original_scenario': \"Jack's family decided to visit the city of Quito, Ecuador this year.\",\n",
       "  'updated_scenario': \"Jack's family decided to visit the city of Quito, Ecuador this year.\",\n",
       "  'consistency': 'Consistent',\n",
       "  'llm_raw_output': \"Consistency: Consistent\\n\\nUpdated scenario: Jack's family decided to visit the city of Quito, Ecuador this year.\\n\\nExplanation: Since there is no information about Jack's age or mobility in the story so far, we can't assume he might not be able to travel. Therefore, visiting a foreign city like Quito does not contradict any previously established facts.\",\n",
       "  'story_so_far': \"Jack is 115 years old and has a family that celebrates his birthday.\\nJack's family decided to visit the city of Quito, Ecuador this year.\"},\n",
       " {'step': 4,\n",
       "  'original_scenario': 'La Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.',\n",
       "  'updated_scenario': '',\n",
       "  'consistency': 'Consistent',\n",
       "  'llm_raw_output': 'To check for consistency, let\\'s analyze the story so far:\\n\\n* Jack is 115 years old: This implies that Jack is likely to be at an advanced age and may have physical limitations or mobility issues.\\n* Jack\\'s family decided to visit the city of Quito, Ecuador this year: Given Jack\\'s age, it seems unlikely that he would be physically able to travel, especially considering the long flight required to get to Quito.\\n\\nNow, let\\'s examine the next scenario:\\n\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.\\n\\nGiven Jack\\'s advanced age and mobility issues implied by his age, visiting La Virgen del Panecillo would likely be challenging for him. However, it doesn\\'t directly contradict any previously verified facts in the story.\\n\\nHowever, considering the context of the previous statement \"Jack\\'s family decided to visit the city of Quito\", we should ensure that this is a realistic expectation given Jack\\'s age.\\n\\nTo make the scenario consistent, let\\'s add a clarification:\\n\\nConsistency: Consistent\\n\\nUpdated scenario:\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill. We can only visit if we take a cable car to get there, which will be arranged by Jack\\'s family for his sake.\\n\\nNote: I\\'ve added a detail about taking a cable car to make it more plausible given Jack\\'s age and mobility issues.',\n",
       "  'story_so_far': \"Jack is 115 years old and has a family that celebrates his birthday.\\nJack's family decided to visit the city of Quito, Ecuador this year.\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.\"},\n",
       " {'step': 5,\n",
       "  'original_scenario': 'La Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill, which is unusual because Quito is known for its flat terrain and is easy to walk through.',\n",
       "  'updated_scenario': \"La Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill, which contrasts with Quito's generally flat terrain and easy walkability.\",\n",
       "  'consistency': 'Inconsistent',\n",
       "  'llm_raw_output': 'Let\\'s analyze the new scenario:\\n\\nThe story so far states:\\n- Jack is 115 years old.\\n- His family decided to visit the city of Quito, Ecuador this year.\\n- La Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.\\n\\nHowever, the new scenario adds: \"which is unusual because Quito is known for its flat terrain and is easy to walk through.\"\\n\\nThis new statement contradicts the existing information about Quito. The original statement said La Virgen del Panecillo overlooks the city from a tall hill, implying that Quito does have some elevation or hills.\\n\\nTo resolve this inconsistency, we can modify the new scenario slightly:\\n\\nThe description of Quito\\'s terrain is incorrect; in fact, it has some significant altitude and can be quite hilly. La Virgen del Panecillo is still a famous landmark in Quito that overlooks the city from a tall hill.\\n\\nHowever, since the task requires minimal edits to preserve meaning and story flow, I will correct just the relevant part:\\n\\nConsistency: Inconsistent\\n\\nUpdated scenario: La Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill, which contrasts with Quito\\'s generally flat terrain and easy walkability.\\n\\nStory so far:\\nJack is 115 years old.\\nJack\\'s family decided to visit the city of Quito, Ecuador this year.\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.',\n",
       "  'story_so_far': \"Jack is 115 years old and has a family that celebrates his birthday.\\nJack's family decided to visit the city of Quito, Ecuador this year.\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill, which contrasts with Quito's generally flat terrain and easy walkability.\"},\n",
       " {'step': 6,\n",
       "  'original_scenario': 'Jack has asthma, which makes it difficult for him to walk far.',\n",
       "  'updated_scenario': '',\n",
       "  'consistency': 'Consistent',\n",
       "  'llm_raw_output': \"Let's analyze the consistency of the new scenario with the story so far.\\n\\nGiven that La Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill, and considering Quito's generally flat terrain and easy walkability, it is likely that visiting this landmark would require some physical effort or transportation.\\n\\nThe new scenario states that Jack has asthma, which makes it difficult for him to walk far. However, since we don't know the severity of his asthma or any potential treatments he may have, let's assume a minimal edit to make the scenario consistent with the story so far.\\n\\nConsistency: Consistent\\n\\nUpdated scenario:\\nJack has mild asthma, which makes it challenging for him to walk far.\\n\\nThe updated scenario is a small, natural edit that preserves meaning and story flow.\",\n",
       "  'story_so_far': \"Jack is 115 years old and has a family that celebrates his birthday.\\nJack's family decided to visit the city of Quito, Ecuador this year.\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill, which contrasts with Quito's generally flat terrain and easy walkability.\\nJack has asthma, which makes it difficult for him to walk far.\"},\n",
       " {'step': 7,\n",
       "  'original_scenario': 'Jack brought inhalers with him on the trip to manage his asthma symptoms.',\n",
       "  'updated_scenario': 'Jack brought inhalers with him on the trip to manage his asthma symptoms.',\n",
       "  'consistency': 'Consistent',\n",
       "  'llm_raw_output': 'Consistency: Consistent\\n\\nUpdated scenario: Jack brought inhalers with him on the trip to manage his asthma symptoms.',\n",
       "  'story_so_far': \"Jack is 115 years old and has a family that celebrates his birthday.\\nJack's family decided to visit the city of Quito, Ecuador this year.\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill.\\nLa Virgen del Panecillo is a famous landmark in Quito that overlooks the city from a tall hill, which contrasts with Quito's generally flat terrain and easy walkability.\\nJack has asthma, which makes it difficult for him to walk far.\\nJack brought inhalers with him on the trip to manage his asthma symptoms.\"}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print reasoning etc from fixing scenarios function (Check explantion & ontology used to figure out llm reasoning)\n",
    "reasoning_data = final_state[\"results\"]\n",
    "reasoning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb877e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved everything to json file\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create a combined export dictionary\n",
    "export_data = {\n",
    "    \"original_story\": final_state.get(\"story\", \"\"),\n",
    "    \"scenarios\": final_state.get(\"scenarios\", []),\n",
    "    \"updated_scenarios\": final_state.get(\"updated_scenarios\", []),\n",
    "    \"fixed_story\": final_state.get(\"fixed_story\", \"\"),\n",
    "    \"reasoning_results\": final_state.get(\"results\", []),\n",
    "}\n",
    "\n",
    "# Save it as JSON\n",
    "with open(\"story+reasoning_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved everything to json file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2046594c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 1/10\n",
      "running 2/10\n",
      "running 3/10\n",
      "running 4/10\n",
      "running 5/10\n",
      "running 6/10\n",
      "running 7/10\n",
      "running 8/10\n",
      "running 9/10\n",
      "running 10/10\n"
     ]
    }
   ],
   "source": [
    "all_runs = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"running {i+1}/10\")\n",
    "    final_state = graph_no_rag.invoke(initial_state)\n",
    "\n",
    "    all_runs.append({\n",
    "        \"scenarios\": final_state.get(\"scenarios\", []),\n",
    "        \"updated_scenarios\": final_state.get(\"updated_scenarios\", []),\n",
    "        \"fixed_story\": final_state.get(\"fixed_story\", \"\")\n",
    "    })\n",
    "\n",
    "\n",
    "export_data = {\n",
    "    \"original_story\": story,\n",
    "    \"iterations\": all_runs\n",
    "}\n",
    "\n",
    "with open(\"base_model_final_story5_10runs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_data, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
